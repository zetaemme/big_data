{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf140ea7",
   "metadata": {},
   "source": [
    "# Initializing pySpark\n",
    "We use the package [findspark](https://pypi.org/project/findspark/) in order to init the `SparkContext`. Since we're working with *[DataFrames](https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame)* we need to init a `SparkSession` too. The pySpark version is **3.2.0**.<br>\n",
    "The MovieLens dataset can be found at the following [link](https://files.grouplens.org/datasets/movielens/ml-1m.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3c86de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"MovieLens Dataset\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)\n",
    "ss = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5bcdb",
   "metadata": {},
   "source": [
    "# Loading the Dataset\n",
    "## Importing .dat files\n",
    "The first step is to load the dataset, we can do this using the Pandas `read_table()` [function](https://pandas.pydata.org/docs/reference/api/pandas.read_table.html).<br>\n",
    "Doing so we obtain 2 DataFrames:\n",
    "* **Movies**: 3883 x 3\n",
    "* **Ratings**: 1000209 x 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e700eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "movies_pd_dataset = pd.read_table(\"data/movies.dat\", \n",
    "                                  delimiter=\"::\", \n",
    "                                  names=[\"MovieID\", \"Title\", \"Genres\"], \n",
    "                                  engine=\"python\")\n",
    "\n",
    "ratings_pd_dataset = pd.read_table(\"data/ratings.dat\", \n",
    "                                   delimiter=\"::\", \n",
    "                                   names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"], \n",
    "                                   engine=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a37f4b",
   "metadata": {},
   "source": [
    "## Creating a Schema\n",
    "In order to simplify the access to our DataFrames we provide a custom [schema](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.StructType.html) to the `createDataFrame()` [function](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html#pyspark.sql.SparkSession.createDataFrame).<br>\n",
    "As a result of this operation is possible to specify which data [type](https://spark.apache.org/docs/latest/api/python/search.html?q=PySpark.sql.types#) we want to use for each column, reducing the space used by our DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c16c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, ShortType, IntegerType\n",
    "\n",
    "movies_schema = StructType([\n",
    "    StructField(\"MovieID\", ShortType(), False),\n",
    "    StructField(\"Title\", StringType(), False),\n",
    "    StructField(\"Genres\", StringType(), False)\n",
    "])\n",
    "\n",
    "ratings_schema = StructType([\n",
    "    StructField(\"UserID\", ShortType(), False),\n",
    "    StructField(\"MovieID\", ShortType(), False),\n",
    "    StructField(\"Rating\", ShortType(), False),\n",
    "    StructField(\"Timestamp\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "movies_dataset = ss.createDataFrame(movies_pd_dataset, schema=movies_schema)\n",
    "ratings_dataset = ss.createDataFrame(ratings_pd_dataset, schema=ratings_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cdeed",
   "metadata": {},
   "source": [
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac952e88",
   "metadata": {},
   "source": [
    "## Query 1\n",
    "Find the *number of ratings* and *distribution* for each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62597ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Number of ratings\n",
    "ratings_number_for_movie = ratings_dataset.groupBy(\"MovieID\")\\\n",
    "                                          .count().orderBy(\"MovieID\")\n",
    "\n",
    "# Distribution\n",
    "sns.displot(data=ratings_number_for_movie.toPandas()[\"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc37ed0f",
   "metadata": {},
   "source": [
    "## Query 2\n",
    "Find the *number of ratings* and *distribution* for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724d539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of ratings\n",
    "ratings_number_for_user = ratings_dataset.groupBy(\"UserID\")\\\n",
    "                                         .count().orderBy(\"UserID\")\n",
    "\n",
    "# Distribution\n",
    "sns.displot(data=ratings_number_for_user.toPandas()[\"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7ddd05",
   "metadata": {},
   "source": [
    "## Query 3\n",
    "Find the *average score* recieved by each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171b281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "ratings_dataset.groupBy(\"MovieID\").agg({\"Rating\": \"mean\"})\\\n",
    "               .orderBy(\"MovieID\")\\\n",
    "               .select(\n",
    "                   \"MovieID\", \n",
    "                   format_number(\"avg(Rating)\", 4).alias(\"Average\")\n",
    "               )\\\n",
    "               .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf917e4",
   "metadata": {},
   "source": [
    "## Query 4\n",
    "Find the *average score* given by each user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8122c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_dataset.groupBy(\"UserID\").agg({\"Rating\": \"mean\"})\\\n",
    "               .orderBy(\"UserID\")\\\n",
    "               .select(\n",
    "                   \"UserID\", \n",
    "                   format_number(\"avg(Rating)\", 4).alias(\"Average\")\n",
    "               )\\\n",
    "               .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcd523e",
   "metadata": {},
   "source": [
    "## Query 5\n",
    "Top **K** movies with at least **R** ratings\n",
    "\n",
    "Corresponding SQL query:\n",
    "```sql\n",
    "SELECT M.MovieID, M.Title, COUNT(R.Ratings) AS RatingsNumber\n",
    "FROM Movies M JOIN Ratings R ON M.MovieID=R.MovieID\n",
    "GROUP BY M.MovieID\n",
    "HAVING RatingsNumber >= R\n",
    "LIMIT K;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c4d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "K = 10\n",
    "R = 20\n",
    "\n",
    "movies_dataset.join(ratings_dataset, \"MovieID\").groupBy(\"MovieID\").count()\\\n",
    "              .orderBy(\"MovieID\").withColumnRenamed(\"count\", \"Number of Ratings\")\\\n",
    "              .filter(col(\"Number of Ratings\") >= R).limit(K).show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "09ecd0348478a1a2cde98c360adc00050098572b8879c23326ed7744dd8a4e3a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pyspark': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
