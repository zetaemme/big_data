{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Inizializzazione pySpark\n",
    "Tramite il package `findspark` inizializzo `SparkContext`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init();\n",
    "\n",
    "import pyspark\n",
    "\n",
    "context = pyspark.SparkContext(appName=\"LID Project\");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils.read import read_fvecs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PCA\n",
    "## Caricamento del Dataset\n",
    "Proviamo in primis a caricare il Dataset, una matrice 10000x128 di `np.float32`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "dataset = read_fvecs(\"data/siftsmall_base.fvecs\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creazione RDD\n",
    "Ora tramite `parallelize()` creiamo l'RDD che conterrà la matrice"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rdd = context.parallelize(dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dati per PCA\n",
    "Per procedere all'esecuzione di PCA ci servono dei dati di partenza quali:\n",
    "* N: Dimensione del dataset\n",
    "* dim: Dimensionalità del dataset\n",
    "* mean: Media del dataset\n",
    "* cov: Matrice di covarianza"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "mean = np.mean(rdd.collect(), axis=0)\n",
    "cov = np.cov(rdd.collect())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Autovettori e Autovalori\n",
    "Calcolo degli autovettori e autovalori a partire da `cov` e sorting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "eigval, eigvec = np.linalg.eig(cov)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting\n",
    "Come è possibile vedere: \n",
    "* Con 12 PC abbiamo rappresentatività per (circa) il 70% del dataset \n",
    "* Con 6 (circa) il 60%\n",
    "* Con 3 tra il 50% ed il 60%"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tot = sum(eigval)\n",
    "\n",
    "var_exp = [(i/tot) for i in sorted(eigval, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.step(range(1, 129),cum_var_exp[:128], where='mid')\n",
    "\n",
    "plt.xlabel('Indice PC')\n",
    "plt.ylabel('Coverage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sorting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sorted_eigvals = sorted(eigval, reverse=True)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('pyspark': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "e65581ca54dfb22a46454c77ee4713c2113dfbbafd30fd3fe1c8f9c06f7e8aa8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}